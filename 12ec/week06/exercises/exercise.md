# Exercise preprocessing and vectorization of text.

Based on this week's lecture and Chapter 10...

Try to take some of the data from last week and

- preprocess them (in different ways)
- vectorize them
- give a (tabular and/or graphical) overview of tokens (unigrams, bigrams, collocations)


Then,

- compare that bottom-up approach with a top-down (keyword or regular-expression based) approach
