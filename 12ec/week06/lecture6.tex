% !TeX document-id = {f19fb972-db1f-447e-9d78-531139c30778}
% !BIB program = biber
\documentclass[compress]{beamer}
\usepackage[T1]{fontenc}
\usetheme[block=fill,subsectionpage=progressbar,sectionpage=progressbar]{metropolis} 

\usepackage{wasysym}
\usepackage{etoolbox}
\usepackage[utf8]{inputenc}

\usepackage{threeparttable}
\usepackage{subcaption}

\usepackage{tikz-qtree}
\setbeamercovered{still covered={\opaqueness<1->{5}},again covered={\opaqueness<1->{100}}}


\usepackage{listings}

\lstset{
	basicstyle=\scriptsize\ttfamily,
	columns=flexible,
	breaklines=true,
	numbers=left,
	%stepsize=1,
	numberstyle=\tiny,
	backgroundcolor=\color[rgb]{0.85,0.90,1}
}



\lstnewenvironment{lstlistingoutput}{\lstset{basicstyle=\footnotesize\ttfamily,
		columns=flexible,
		breaklines=true,
		numbers=left,
		%stepsize=1,
		numberstyle=\tiny,
		backgroundcolor=\color[rgb]{.7,.7,.7}}}{}


\lstnewenvironment{lstlistingoutputtiny}{\lstset{basicstyle=\tiny\ttfamily,
		columns=flexible,
		breaklines=true,
		numbers=left,
		%stepsize=1,
		numberstyle=\tiny,
		backgroundcolor=\color[rgb]{.7,.7,.7}}}{}



\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage[style=apa, backend = biber]{biblatex}
\DeclareLanguageMapping{american}{american-UoN}
\addbibresource{../../bdaca.bib}
\renewcommand*{\bibfont}{\tiny}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows,matrix}
\usepackage{multicol}

\usepackage{subcaption}

\usepackage{booktabs}
\usepackage{graphicx}



\makeatletter
\setbeamertemplate{headline}{%
	\begin{beamercolorbox}[colsep=1.5pt]{upper separation line head}
	\end{beamercolorbox}
	\begin{beamercolorbox}{section in head/foot}
		\vskip2pt\insertnavigation{\paperwidth}\vskip2pt
	\end{beamercolorbox}%
	\begin{beamercolorbox}[colsep=1.5pt]{lower separation line head}
	\end{beamercolorbox}
}
\makeatother



\setbeamercolor{section in head/foot}{fg=normal text.bg, bg=structure.fg}



\newcommand{\question}[1]{
	\begin{frame}[plain]
		\begin{columns}
			\column{.3\textwidth}
			\makebox[\columnwidth]{
				\includegraphics[width=\columnwidth,height=\paperheight,keepaspectratio]{../../pictures/mannetje.png}}
			\column{.7\textwidth}
			\large
			\textcolor{orange}{\textbf{\emph{#1}}}
		\end{columns}
\end{frame}}




\title[Big Data and Automated Content Analysis]{\textbf{Big Data \& Automated Content Analysis} \\ Week 6 -- Wednesday: »Text as data«}
\author[Damian Trilling]{Damian Trilling \\ ~ \\ \footnotesize{d.c.trilling@uva.nl \\@damian0604} \\ \url{www.damiantrilling.net}}
\date{10 March 2021}
\institute[UvA]{Afdeling Communicatiewetenschap \\Universiteit van Amsterdam}

\begin{document}
	
	\begin{frame}{}
		\titlepage
	\end{frame}
	
	\begin{frame}{Today}
		\tableofcontents
	\end{frame}
	
	
\begin{frame}[standout]	
	How did it go? And some first feedback on the exam.
\end{frame}
	
	
	
	\begin{frame}[standout]	
		Everything clear from last week?
	\end{frame}
	
\section{The bag-of-words (BOW) model}

\subsection{General idea}

\begin{frame}[fragile]{A text as a collections of word}

Let us represent a string 
\begin{lstlisting}
t = "This this is is is a test test test"
\end{lstlisting}
like this:\\
\begin{lstlisting}
from collections import Counter
print(Counter(t.split()))
\end{lstlisting}
\begin{lstlistingoutput}
Counter({'is': 3, 'test': 3, 'This': 1, 'this': 1, 'a': 1})
\end{lstlistingoutput}

\pause 
Compared to the original string, this representation
\begin{itemize}
	\item is less repetitive
	\item preserves word frequencies
	\item but does \emph{not} preserve word order
	\item can be interpreted as a vector to calculate with (!!!)
\end{itemize}

\tiny{\emph{Of course, still a lot of stuff to fine-tune\ldots}  (for example, This/this)}
\end{frame}



\begin{frame}{From vector to matrix}
If we do this for multiple texts, we can arrange the vectors in a table.

t1 = "This this is is is a test test test" \newline
t2 = "This is an example"

\begin{tabular}{| c|c|c|c|c|c|c|c|}
	\hline
	& a & an & example & is & this & This & test \\
	\hline
	\emph{t1} & 1 & 0 & 0 & 3 & 1 & 1 & 3 \\
	\emph{t2} &0 & 1 & 1 & 1 & 0 & 1 & 0 \\
	\hline
\end{tabular}
\end{frame}


\question{What can you do with such a matrix? Why would you want to represent a collection of texts in such a way?}


\begin{frame}{The cell entries: raw counts versus tf$\cdot$idf scores}
\begin{itemize}
	\item In the example, we entered simple counts (the ``term frequency'')
\end{itemize}
\end{frame}

\question{But are all terms equally important?}


\begin{frame}{The cell entries: raw counts versus tf$\cdot$idf scores}
	\begin{itemize}
		\item In the example, we entered simple counts (the ``term frequency'')
		\item But does a word that occurs in almost all documents contain much information?
		\item And isn't the presence of a word that occurs in very few documents a pretty strong hint?
		\item<2-> \textbf{Solution: Weigh by \emph{the number of documents in which the term occurs at least once) (the ``document frequency'')}} 
	\end{itemize}
\onslide<3->{
$\Rightarrow$ we multiply the ``term frequency'' (tf) by the inverse document frequency (idf)

\tiny{(usually with some additional logarithmic transformation and normalization applied, see \url{https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html})}
}
\end{frame}

\begin{frame}{Is tf$\cdot$idf always better?}
It depends.

\begin{itemize}
	\item Ultimately, it's an empirical question which works better ($\rightarrow$ weeks on machine learning)
	\item In many scenarios,  ``discounting'' too frequent words and ``boosting'' rare words makes a lot of sense (most frequent words in a text can be highly un-informative)
	\item Beauty of raw tf counts, though: interpretability + describes document in itself, not in relation to other documents
\end{itemize}
\end{frame}


\begin{frame}{Internal representations}
\begin{block}{Sparse vs dense matrices}
\begin{itemize}
	\item Most are not \emph{not} contained in a given document
	\item $\rightarrow$ tens of thousands of columns (terms), and one row per document
	\item Filling all cells is inefficient \emph{and} can make the matrix too large to fit in memory (!!!)
	\item Solution: store only non-zero values with their coordinates! (sparse matrix)
	\item dense matrix (or dataframes) not advisable, only for toy examples
\end{itemize}
\end{block}
\end{frame}



\begin{frame}{Internal representations}
	\begin{alertblock}{Little over-generalizing R vs Python remark}
		Among R users, it is very common to manually inspect document-term matrices, and many operations are done directly on them. In Python, they are more commonly seen as a means to an end (mostly, as input for machine learning). 
		
		Many R modules convert to dense matrices: really problematic for larger datasets!
	\end{alertblock}
	
\end{frame}






\section{Getting a clean BOW representation}

\begin{frame}{Room for improvement}
\begin{description}
	\item[tokenization] How do we (best) split a sentence into tokens (terms, words)?
	\item[pruning] How can we remove unneccessary words?
	\item[lemmatization] How can we make sure that slight variations of the same word are not counted differently?

\end{description}
\end{frame}

\subsection{Better tokenization}

\begin{frame}[fragile]{OK, good enough, perfect?}
\begin{block}{.split()}
\begin{itemize}
	\item space $\rightarrow$ new word
	\item no further processing whatsoever
	\item thus, only works well if we do a preprocessing outselves (e.g., remove punctuation)
\end{itemize}
\end{block}
\begin{lstlisting}
docs = ["This is a text",  "I haven't seen John's derring-do. Second sentence!"]
tokens = [d.split() for d in docs]
\end{lstlisting}
\begin{lstlistingoutputtiny}
[['This', 'is', 'a', 'text'], ['I', "haven't", 'seen', "John's", 'derring-do.', 'Second', 'sentence!']]
\end{lstlistingoutputtiny}
\end{frame}


\begin{frame}[fragile]{OK, good enough, perfect?}
	\begin{block}{Tokenizers from the NLTK pacakge}
		\begin{itemize}
			\item multiple improved tokenizers that can be used instead of .split()
			\item e.g., Treebank tokenizer:
			\begin{itemize}
				\item split standard contractions ("don't")
				\item deals with punctuation
			\end{itemize}			
		\end{itemize}
	\end{block}
\begin{lstlisting}
from nltk.tokenize import TreebankWordTokenizer
tokens = [TreebankWordTokenizer().tokenize(d) for d in docs]
\end{lstlisting}
\begin{lstlistingoutputtiny}
[['This', 'is', 'a', 'text'],  ['I', 'have', "n't", 'seen', 'John', "'s", 'derring-do.', 'Second', 'sentence', '!']]
\end{lstlistingoutputtiny}
\tiny{Notice the failure to split the \texttt{.} at the end of the first sentence in the second doc. That's because TreebankWordTokenizer expects \emph{sentences} as input. See book for a solution.\\}
\end{frame}


\begin{frame}[standout]
OK, so we can tokenize with a list comprehension (and that's often a good idea!). But what if we want to \emph{directly} get a DTM instead of lists of tokens?
\end{frame}


\begin{frame}[fragile]{OK, good enough, perfect?}
	\begin{block}{scikit-learn's CountVectorizer (default settings)}
		\begin{itemize}
			\item applies lowercasing
			\item deals with punctuation etc. itself
			\item minimum word length $>1$
			\item more technically, tokenizes using this regular expression: \texttt{r"(?u)\textbackslash b\textbackslash w\textbackslash w+\textbackslash b"} \footnote{?u = support unicode, \textbackslash b = word boundary}
		\end{itemize}
	\end{block}
\begin{lstlisting}
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
dtm_sparse = cv.fit_transform(docs)
\end{lstlisting}
\end{frame}


\begin{frame}{OK, good enough, perfect?}
	\begin{block}{CountVectorizer supports more}
		\begin{itemize}
			\item stopword removal
			\item custom regular expression
			\item or even using an external tokenizer
			\item ngrams instead of unigrams
		\end{itemize}
	\end{block}
			\tiny{see \url{https://scikit-learn.org/stable/modules/generated/sklearn.feature\_extraction.text.CountVectorizer.html}}

\pause
\begin{alertblock}{Best of both worlds}
\textbf{Use the Count vectorizer with a NLTK-based external tokenizer! (see book)}
\end{alertblock}
\end{frame}



\subsection{Stopword removal}



\begin{frame}{Stopword removal}
	\begin{block}{What are stopwords?}
		\begin{itemize}
			\item Very frequent words with little inherent meaning
			\item \texttt{the, a, he, she, \ldots}
			\item context-dependent: if you are interested in gender, \texttt{he} and \texttt{she} are no stopwords. 
			\item Many existing lists as basis
		\end{itemize}
	\end{block}

When using the CountVectorizer, we can simply provide a stopword list. 

But we can also remove stopwords ``by hand'' (next slide):
\end{frame}

\begin{frame}[fragile]{Stopword removal}
\begin{lstlisting}
from nltk.corpus import stopwords
mystopwords = stopwords.words("english")
mystopwords.extend(["test", "this"])

def tokenize_clean(s, stoplist):
    cleantokens = []
    for w in TreebankWordTokenizer().tokenize(s):
        if w.lower() not in stoplist:
            cleantokens.append(w)
    return cleantokens

tokens = [tokenize_clean(d, mystopwords) for d in docs]
\end{lstlisting}
\begin{lstlistingoutputtiny}
[['text'], ["n't", 'seen', 'John', 'derring-do.', 'Second', 'sentence', '!']]
\end{lstlistingoutputtiny}

\begin{alertblock}{You can do more!}
For instance, in line 8, you could add an \texttt{or} statement to also exclude punctuation.
\end{alertblock}

\end{frame}





\subsection{Pruning}

\begin{frame}{General idea}
\begin{itemize}
	\item Idea behind both stopword removal and tf$\cdot$idf: too frequent words are uninformative
	\item<2-> (possible) downside stopword removal: a priori list, does not take empirical frequencies in dataset into account
	\item<3-> (possible) downside tf$\cdot$idf: does not reduce number of features
\end{itemize}

\onslide<4->{Pruning: remove all features (tokens) that occur in less than X or more than X of the documents}
\end{frame}

\begin{frame}[fragile, plain]
CountVectorizer, only stopword removal
\begin{lstlisting}
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
myvectorizer = CountVectorizer(stop_words=mystopwords)
\end{lstlisting}

CountVectorizer, better tokenization, stopword removal (pay attention that stopword list uses same tokenization!):
\begin{lstlisting}
myvectorizer = CountVectorizer(tokenizer = TreebankWordTokenizer().tokenize, stop_words=mystopwords)
\end{lstlisting}

Additionally remove words that occur in more than 75\% or less than $n=2$ documents:
\begin{lstlisting}
myvectorizer = CountVectorizer(tokenizer = TreebankWordTokenizer().tokenize, stop_words=mystopwords, max_df=.75, min_df=2)
\end{lstlisting}

All togehter: tf$\cdot$idf, explicit stopword removal, pruning
\begin{lstlisting}
myvectorizer = TfidfVectorizer(tokenizer = TreebankWordTokenizer().tokenize, stop_words=mystopwords, max_df=.75, min_df=2)
\end{lstlisting}


\end{frame}


\question{What is ``best''? Which (combination of) techniques to use, and how to decide?}



\subsection{Stemming and lemmatization}


\begin{frame}[fragile]{Stemming and lemmatization}
\begin{itemize}
\item Stemming: reduce words to its stem by removing last part (drinking $\rightarrow$ drink)
\item Lemmatization: find word that you would need to look up in a dictionary (drinking $\rightarrow$ drink, but also went $\rightarrow$ go)
\item stemming is simpler than lemmatization
\item lemmatization often better
\end{itemize}
\pause

Example below: tokenization and lemmatization with \texttt{spacy} in one go:
\begin{lstlisting}
import spacy
nlp = spacy.load('en')   # potentially you need to install the language model first
lemmatized_tokens = [[token.lemma_  for token in nlp(doc)] for doc in docs]
\end{lstlisting}
\begin{lstlistingoutputtiny}
[['this', 'be', 'a', 'text'], ['-PRON-', 'have', 'not', 'see', 'John', "'s", 'derring', '-', 'do', '.', 'second', 'sentence', '!']]
\end{lstlistingoutputtiny}
\end{frame}



\section{The order of preprocessing steps}

\begin{frame}{Option 1}
\begin{block}{Preprocessing only through Vectorizer}
``Just use CountVectorizer or Tfidfvectorizer with the appropriate options.''	
\begin{itemize}
	\item pro: No double work, efficient if your main goal is a sparse matrix (for ML?) anyway
	\item con: you cannot ``see'' the preprocessed texts
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]{Option 2}
	\begin{block}{Extensive preprocessing without Vectorizer}
``Remove stopwords, punctuation etc. and store in a string with spaces''

\begin{lstlisting}
cleaneddocs = [" ".join(re.findall(r"\w\w+", d)).lower() for d in docs]
cleaneddocswithoutstopwords = [" ".join([w for w in d.split() if w not in mystopwords]) for d in cleaneddocs]
\end{lstlisting}
\begin{lstlistingoutputtiny}
['this is text', 'haven seen john derring do second sentence']
['text', 'seen john derring second sentence']	
\end{lstlistingoutputtiny}
{\tiny{Yes, this list comprehension looks scary -- you can make a more elaborate for loop instead}}
	
\begin{itemize}
	\item pro: you can read (and store!) the preprocessed docs
	\item pro: even the most stupid vectorizer (or wordcloud tool) can split the resulting string later on
	\item con: potentially double work (for you \emph{and} the computer)
\end{itemize}
\end{block}
\end{frame}


\question{How would you do it?}

\begin{frame}[plain]
I tend to go for Option 2 because
\begin{itemize}
	\item I like to inspect a sample of the documents
	\item I can re-use the cleaned docs irrespective of the Vectorizer
\end{itemize}

But sometimes, I opt of Option 1 instead because
\begin{itemize}
	\item I want to systematically compare the effect of different choices in a machine learning pipeline (then I can simply vary the vectorizer instead of the data)
	\item I want to use techniques that are geared towards little or no preprocessing (deep learning)
\end{itemize}

\end{frame}


\section{How further?}


\begin{frame}{Main takeaway}

\begin{itemize}
	\item It matters how you transform your text into numbers (``vectorization'').
	\item Preprocessing matters, be able to make informed choices.
	\item Keep this in mind when we will discuss Machine Learning! It will come back throughout Part II!
\end{itemize}

\begin{itemize}
	\item Once you vectorized your texts, you can do all kinds of calculations (random example: get the cosine similarity between two texts)
\end{itemize}

\end{frame}


\begin{frame}{More NLP}
\begin{description}
	\item[$n$-grams] Consider using $n$-grams instead of unigrams
	\item[collocations]  $n$grams that appear more frequently than expected
	\item[POS-tagging] grammatical function (``part-of-speach'') of tokens
	\item[NER] named entity recognition (persons, organizations, locations)
\end{description}
\end{frame}

\begin{frame}{More NLP}
I \textbf{really} recommend looking into spacy (\url{https://spacy.io}) for advanced natural language processing, such as part-of-speech-tagging and named entity recogntion.
\end{frame}


\begin{frame}{Friday}
\begin{block}{Based on today's lecture and Chapter 10\ldots}
Try to take some of the data from last week and
\begin{itemize}
	\item preprocess them (in different ways)
	\item vectorize them
	\item give a (tabular and/or graphical) overview of tokens (unigrams, bigrams, collocations).
\end{itemize}
Then,
\begin{itemize}
	\item compare that bottom-up approach with a top-down (keyword or regular-expression based) approach
\end{itemize}

\end{block}
\end{frame}


%\begin{frame}[plain]
%	\printbibliography
%\end{frame}




\end{document}



